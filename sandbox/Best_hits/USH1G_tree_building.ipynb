{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File path to the list of species\n",
    "```\n",
    "/home/eeb177-student/Desktop/eeb-177/project/sandbox/WHRN-Harm-PDZD7_comparisson/taxa_list_test_reduced_formated.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab the taxa ids of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_list_taxa_file = open(\"/home/eeb177-student/Desktop/eeb-177/project/sandbox/WHRN-Harm-PDZD7_comparisson/taxa_list_test_reduced_formated.txt\", \"r\").readlines()\n",
    "test_taxa_list = [] #set up a holder to append into\n",
    "for line in test_list_taxa_file:\n",
    "    test_taxa_list.append(line.split(\",\")[1].strip(\"\\n\")) #get the ids only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['946362', '192875', '10228', '400682', '9606', '45351', '6087', '7668', '6239', '7227', '37653', '7574']\n"
     ]
    }
   ],
   "source": [
    "print(test_taxa_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_and_getID_best_hits(taxa_id_list, file_format_name):\n",
    "    #  goes through the output of a BLAST xml file \n",
    "    #  finds the best result for the taxa's submited\n",
    "    from Bio.Blast import NCBIXML\n",
    "    from Bio.SeqRecord import SeqRecord\n",
    "    #import the required libraries\n",
    "    \n",
    "    for taxa_id in taxa_id_list:\n",
    "        file_name = str(\"USH_Search_\" + \"txid\" +taxa_id + \"[ORGN].xml\")\n",
    "        #this sets the handle, without having to format every single name of the input files\n",
    "        #added a txid, so we only need to grab only the number from the file, nothing else\n",
    "        \n",
    "        result_handle = open(str(\"/home/eeb177-student/Desktop/eeb-177/project/sandbox/Testrun_multi_genes_same_org/Batch_run_workspace/\"+file_name), \"r\") #sets the result handle to read the results from\n",
    "        #i've altered this to be able to find the genes without having them in the same directory\n",
    "        \n",
    "        blast_records = NCBIXML.parse(result_handle)\n",
    "        #need to use parse if it has multiple records in it\n",
    "        for blast_record in blast_records:\n",
    "            org_desig=file_name.split(\"_\")[2].split(\"[\")[0].replace(\" \", \"_\")\n",
    "            #properly formats the taxa id so we can use it to name things\n",
    "            \n",
    "            homo_sapiens = \"[Homo sapiens]\"\n",
    "            blast_query=blast_record.query\n",
    "            if homo_sapiens in blast_query:\n",
    "                gene_name=blast_record.query.split(\"|\")[4].split(\"[\")[0].replace(\" \", \"_\").replace(\"_protein\", \"\").replace(\"_isoform_b3\", \"\")\n",
    "                formated_gene_name = gene_name[1:-1]\n",
    "            else:\n",
    "                formated_gene_name=blast_query.split(\"|\")[4].split(\"_\")[0]\n",
    "            #this conditional properly formats the gene name so we can use it to name things\n",
    "                #Basically there are 2 formats of sequence names I used to do the search\n",
    "                #this statement switches the naming convention depending on which is used\n",
    "                #it's not very general, so in the future I will make sure to properly name my search sequences\n",
    "                #that should make this statement unnecessary \n",
    "            \n",
    "            \n",
    "            output_file_name=(\"gene_\"+formated_gene_name +\"_\"+file_format_name+ \"_summary.csv\")\n",
    "            #changed this line so we can add the extra variable so we don't have to redefine the function every time we run it\n",
    "        \n",
    "            output_file=open(output_file_name, \"a\")\n",
    "            \n",
    "            #this puts together a nicely named file for each gene and species\n",
    "            \n",
    "            alignment_file_contents=\"\"\n",
    "            #setting up an empty list to hold the output of the following loop\n",
    "            for alignment in blast_record.alignments[:1]:\n",
    "                #change the number here to determine how many of the hits you the number specifies \n",
    "                #if it is 3 that you are grabing the 3 best hits\n",
    "\n",
    "                score_counter=[]\n",
    "                e_val_counter=[]\n",
    "\n",
    "                for hsp in alignment.hsps:\n",
    "                    evalue=hsp.expect\n",
    "                    e_val_counter.append(evalue)\n",
    "                \n",
    "                seq_designation = alignment.title.split(\"|\")\n",
    "                gi_number = seq_designation[1]\n",
    "                ref_number = seq_designation[3]\n",
    "                annotation = seq_designation[4]\n",
    "                #breaking apart the annoation of the sequence name\n",
    "                #otherwise we'd have way too many delimiters within delimiters\n",
    "                output_line = str(gi_number + \",\"+ \n",
    "                                  ref_number + \",\" +\n",
    "                                  annotation+ \",\" +\n",
    "                                  str(min(e_val_counter)) + \"\\n\")\n",
    "                #this sets up the output for each alignment\n",
    "                #had to alter the line breaks and parenthesis positioning\n",
    "                \n",
    "                alignment_file_contents = alignment_file_contents + output_line\n",
    "                #this writes what we came up with to the holder string\n",
    "            output_file.write(alignment_file_contents)\n",
    "            #writing the output to the alignment file\n",
    "            output_file.close()\n",
    "        result_handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parse_and_getID_best_hits(test_taxa_list, \"best_hit_reduced_taxa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Let's make a nice little script to pull out the relevant GI's\n",
    "\n",
    "tried to make a bash script work, let's just go with a python script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_gi_from_summary(input_file_name):\n",
    "    #let's format the file name and the contents\n",
    "    \n",
    "    intermediate_name = input_file_name.split(\"_\")[1:-1]\n",
    "    #gets rid of uneccessary parts of the name\n",
    "    final_name = \"seqs_\" + \"_\".join(intermediate_name)+ \"_gi.txt\"\n",
    "    #this gives us the formated output name\n",
    "    \n",
    "    output_file = open(final_name, \"w\")\n",
    "    #open the file to write to it\n",
    "    \n",
    "    \n",
    "    input_file_contents = open(input_file_name, \"r\").read()\n",
    "    for line in input_file_contents.split(\"\\n\")[:-1]:\n",
    "    # the -1 gets around the extra line at the end of the file \n",
    "    #so everything is ready to input for the next process\n",
    "        output_file.write(str(line.split(\",\")[0] + \"\\n\"))\n",
    "    output_file.close()\n",
    "    # \n",
    "    #    for line in input_file_contents.split(\"\\n\"):\n",
    "#        output_file_contents = print(line)\n",
    "#    print(output_file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_gi_from_summary(\"gene_cadherin_23_best_hit_reduced_taxa_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gene_list =(\"gene_cadherin_23_best_hit_reduced_taxa_summary.csv\",\n",
    "            \"gene_CIB2_best_hit_reduced_taxa_summary.csv\",\n",
    "            \"gene_Clarin_1_best_hit_reduced_taxa_summary.csv\",\n",
    "            \"gene_GPR98_best_hit_reduced_taxa_summary.csv\",\n",
    "            \"gene_GPR98_best_hit_reduced_taxa_summary.csv\",\n",
    "            \"gene_harmonin_best_hit_reduced_taxa_summary.csv\",\n",
    "            \"gene_myosin_VIIA_best_hit_reduced_taxa_summary.csv\",\n",
    "            \"gene_PDZD7_best_hit_reduced_taxa_summary.csv\",\n",
    "            \"gene_protocadherin_15_best_hit_reduced_taxa_summary.csv\",\n",
    "            \"gene_USH1G_best_hit_reduced_taxa_summary.csv\",\n",
    "            \"gene_USH2A_best_hit_reduced_taxa_summary.csv\",\n",
    "            \"gene_WHRN_best_hit_reduced_taxa_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for gene in gene_list:\n",
    "    get_gi_from_summary(gene)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Switch to the shell\n",
    "going to use grep to grab all the things with _gi in the name\n",
    "\n",
    "```\n",
    "ls | grep \"_gi.txt\" > organization_best_hits_gi_filenames.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['seqs_cadherin_23_best_hit_reduced_taxa_gi.txt\\n', 'seqs_CIB2_best_hit_reduced_taxa_gi.txt\\n', 'seqs_Clarin_1_best_hit_reduced_taxa_gi.txt\\n', 'seqs_GPR98_best_hit_reduced_taxa_gi.txt\\n', 'seqs_harmonin_best_hit_reduced_taxa_gi.txt\\n', 'seqs_myosin_VIIA_best_hit_reduced_taxa_gi.txt\\n', 'seqs_PDZD7_best_hit_reduced_taxa_gi.txt\\n', 'seqs_protocadherin_15_best_hit_reduced_taxa_gi.txt\\n', 'seqs_USH1G_best_hit_reduced_taxa_gi.txt\\n', 'seqs_USH2A_best_hit_reduced_taxa_gi.txt\\n', 'seqs_WHRN_best_hit_reduced_taxa_gi.txt\\n']\n"
     ]
    }
   ],
   "source": [
    "gi_files_to_download = open(\"organization_best_hits_gi_filenames.txt\", \"r\").readlines()\n",
    "print(gi_files_to_download)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok, well that is working after a lot of work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_seqs_from_list_and_autoname(in_filename):    \n",
    "    with open(in_filename, \"r\") as query_file:\n",
    "        query_ids = query_file.read().splitlines()\n",
    "    #open the file, grab the ids\n",
    "\n",
    "    from Bio import Entrez\n",
    "    Entrez.email = \"hspeck@ucla.edu\"\n",
    "    # import Bio Entrez, let NCBI know who I am\n",
    "\n",
    "    search_results = Entrez.read(Entrez.epost(db =\"protein\", \n",
    "                                              id = \",\".join(query_ids)))  # needs the id's to be as a list separated by commas\n",
    "    #upload the IDs to NCBI\n",
    "    webenv_return = search_results[\"WebEnv\"]\n",
    "    query_key_return = search_results[\"QueryKey\"]\n",
    "    #grab the relevant variables to call on the stuff we posted\n",
    "\n",
    "    count = len(query_ids)\n",
    "    #assigns the count based on the number of sequences we searched for\n",
    "\n",
    "    from urllib.error import HTTPError\n",
    "    # load required library for the try and except conditions\n",
    "\n",
    "    batch_size = 20\n",
    "    #this determines how many things we retrieve and write to the file\n",
    "    #can safely be larger in the future\n",
    "\n",
    "    out_filename = str(in_filename[:-7]+\".fasta\")\n",
    "    #let's alter the file name a bit so we can easily move on\n",
    "    \n",
    "    #attempting to rename the file based on the input of the original file\n",
    "    out_handle = open(out_filename, \"w\")\n",
    "    #open file to write to\n",
    "\n",
    "    for start in range(0, count, batch_size):\n",
    "        end = min(count, start+batch_size)\n",
    "        print(\"Going to download record %i to %i\" % (start+1, end))\n",
    "        attempt = 0\n",
    "        while attempt < 3:\n",
    "            attempt += 1\n",
    "            try:\n",
    "                fetch_handle = Entrez.efetch(db=\"protein\", #says which db\n",
    "                                             rettype=\"fasta\", #says what format the data should be in\n",
    "                                             retmode=\"text\", #what the output should be\n",
    "                                             retstart = start, #say what range of results you want returned\n",
    "                                             retmax = batch_size, #say end of range of results want returned\n",
    "                                             webenv = webenv_return, #specify the info we uploaded with ePost\n",
    "                                             query_key = query_key_return)\n",
    "            except HTTPError as err:\n",
    "                if 500 <= err.code <= 599:\n",
    "                    print(\"Recieved error from server %s\" % err)\n",
    "                    print(\"Attempt %i of 3\" % attempt)\n",
    "                    time.sleep\n",
    "                else:\n",
    "                    raise\n",
    "        data = fetch_handle.read()\n",
    "        fetch_handle.close()\n",
    "        out_handle.write(data)\n",
    "    out_handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "organization_best_hits_gi.txt\n",
      "seqs_cadherin_23_best_hit_reduced_taxa_gi.txt\n",
      "seqs_CIB2_best_hit_reduced_taxa_gi.txt\n",
      "seqs_Clarin_1_best_hit_reduced_taxa_gi.txt\n",
      "seqs_GPR98_best_hit_reduced_taxa_gi.txt\n",
      "seqs_harmonin_best_hit_reduced_taxa_gi.txt\n",
      "seqs_myosin_VIIA_best_hit_reduced_taxa_gi.txt\n",
      "seqs_PDZD7_best_hit_reduced_taxa_gi.txt\n",
      "seqs_protocadherin_15_best_hit_reduced_taxa_gi.txt\n",
      "seqs_USH1G_best_hit_reduced_taxa_gi.txt\n",
      "seqs_USH2A_best_hit_reduced_taxa_gi.txt\n",
      "seqs_WHRN_best_hit_reduced_taxa_gi.txt\n"
     ]
    }
   ],
   "source": [
    "for file in gi_files_to_download:\n",
    "    print(file.replace(\"\\n\", \"\"))\n",
    "    # this is needed to actually input the right thingy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "muscle_align_cadherin_23_best_hit_reduced_taxa.txt\n",
      "muscle_align_CIB2_best_hit_reduced_taxa.txt\n",
      "muscle_align_Clarin_1_best_hit_reduced_taxa.txt\n",
      "muscle_align_GPR98_best_hit_reduced_taxa.txt\n",
      "muscle_align_harmonin_best_hit_reduced_taxa.txt\n",
      "muscle_align_myosin_VIIA_best_hit_reduced_taxa.txt\n",
      "muscle_align_PDZD7_best_hit_reduced_taxa.txt\n",
      "muscle_align_protocadherin_15_best_hit_reduced_taxa.txt\n",
      "muscle_align_USH1G_best_hit_reduced_taxa.txt\n",
      "muscle_align_USH2A_best_hit_reduced_taxa.txt\n",
      "muscle_align_WHRN_best_hit_reduced_taxa.txt\n"
     ]
    }
   ],
   "source": [
    "#testing the rename function\n",
    "\n",
    "for file_name in gi_files_to_download:\n",
    "    intermediate_name = file_name.split(\"_\")[1:-1]\n",
    "    muscle_align = \"muscle_align_\" + \"_\".join(intermediate_name) + \".txt\"\n",
    "    print(muscle_align)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make it download and run MUSCLE automatically\n",
    "\n",
    "so we need to incorporate the muscle step into the download\n",
    "\n",
    "preferably wed rename the files at the same time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_seqs_from_list_autoname_align(in_filename):    \n",
    "    with open(in_filename, \"r\") as query_file:\n",
    "        query_ids = query_file.read().splitlines()\n",
    "    #open the file, grab the ids\n",
    "\n",
    "    from Bio import Entrez\n",
    "    Entrez.email = \"hspeck@ucla.edu\"\n",
    "    # import Bio Entrez, let NCBI know who I am\n",
    "\n",
    "    search_results = Entrez.read(Entrez.epost(db =\"protein\", \n",
    "                                              id = \",\".join(query_ids)))  # needs the id's to be as a list separated by commas\n",
    "    #upload the IDs to NCBI\n",
    "    webenv_return = search_results[\"WebEnv\"]\n",
    "    query_key_return = search_results[\"QueryKey\"]\n",
    "    #grab the relevant variables to call on the stuff we posted\n",
    "\n",
    "    count = len(query_ids)\n",
    "    #assigns the count based on the number of sequences we searched for\n",
    "\n",
    "    from urllib.error import HTTPError\n",
    "    # load required library for the try and except conditions\n",
    "\n",
    "    batch_size = 20\n",
    "    #this determines how many things we retrieve and write to the file\n",
    "    #can safely be larger in the future\n",
    "\n",
    "    out_filename = str(in_filename[:-7]+\".fasta\")\n",
    "    #let's alter the file name a bit so we can easily move on\n",
    "    \n",
    "    #attempting to rename the file based on the input of the original file\n",
    "    out_handle = open(out_filename, \"w\")\n",
    "    #open file to write to\n",
    "\n",
    "    for start in range(0, count, batch_size):\n",
    "        end = min(count, start+batch_size)\n",
    "        print(\"Going to download record %i to %i\" % (start+1, end))\n",
    "        attempt = 0\n",
    "        while attempt < 3:\n",
    "            attempt += 1\n",
    "            try:\n",
    "                fetch_handle = Entrez.efetch(db=\"protein\", #says which db\n",
    "                                             rettype=\"fasta\", #says what format the data should be in\n",
    "                                             retmode=\"text\", #what the output should be\n",
    "                                             retstart = start, #say what range of results you want returned\n",
    "                                             retmax = batch_size, #say end of range of results want returned\n",
    "                                             webenv = webenv_return, #specify the info we uploaded with ePost\n",
    "                                             query_key = query_key_return)\n",
    "            except HTTPError as err:\n",
    "                if 500 <= err.code <= 599:\n",
    "                    print(\"Recieved error from server %s\" % err)\n",
    "                    print(\"Attempt %i of 3\" % attempt)\n",
    "                    time.sleep\n",
    "                else:\n",
    "                    raise\n",
    "        data = fetch_handle.read()\n",
    "        fetch_handle.close()\n",
    "        out_handle.write(data)\n",
    "    out_handle.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "    #now to run MUSCLE on it\n",
    "    raw_seq_input = out_filename\n",
    "    intermediate_name = in_filename.split(\"_\")[1:-1] \n",
    "    #grabs the core of the name\n",
    "    muscle_align_name = \"muscle_align_\" + \"_\".join(intermediate_name) + \".txt\" \n",
    "    # generates a named file for the muscle alginment output\n",
    "    \n",
    "    from Bio.Align.Applications import MuscleCommandline\n",
    "    #imports the relevant biopython module for python\n",
    "    muscle_cline = MuscleCommandline(input = raw_seq_input, \n",
    "                                 out = muscle_align_name)\n",
    "    stdout, stder = muscle_cline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOW TO run the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to download record 1 to 12\n",
      "Going to download record 1 to 12\n",
      "Going to download record 1 to 5\n",
      "Going to download record 1 to 12\n",
      "Going to download record 1 to 12\n",
      "Going to download record 1 to 12\n",
      "Going to download record 1 to 12\n",
      "Going to download record 1 to 12\n",
      "Going to download record 1 to 12\n",
      "Going to download record 1 to 12\n",
      "Going to download record 1 to 12\n"
     ]
    }
   ],
   "source": [
    "for file in gi_files_to_download:\n",
    "    formated_input = file.replace(\"\\n\", \"\")\n",
    "    download_seqs_from_list_autoname_align(formated_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
